---
layout: post
title: ceph 读流程（2）
date: 2016-09-20 22:21:40
categories: ceph-internal
tag: ceph
excerpt: 接续介绍ceph的读流程
---

# 前言
上篇介绍了从读请求到达开始讲起，一直讲到了进入消息队列。以及负责处理读请求的消息队列和线程池的渊源由来。本文重点focus在从队列中取出读请求，后续如何处理该请求。

# 流程图

![](/assets/ceph_internals/ceph_read_process.png)

上图是从运行队列开始取出消息，一直到FileStore从磁盘中读到文件的内容的过程。箭头指向并非下一个工序的一次，而是A调用了B的含义。

很多函数很复杂，并非仅仅处理读，比如ReplicatedPG::execute_ctx不仅仅处理读，也处理写以及其他的请求。函数流程异常复杂。我们从读入手的目的，是熟悉相关的调用流程，为后面更复杂的写入，以及更复杂的异常流程做好铺垫，


# 相关的debug log

我们打开debug log，写入一个4M的文件file_01，从另外一个存储节点读取它


    root@BEAN-2:/var/share/ezfs/shareroot/NAS# cephfs file_01 map
    WARNING: This tool is deprecated.  Use the layout.* xattrs to query and modify layouts.
        FILE OFFSET                    OBJECT        OFFSET        LENGTH  OSD
                  0      100000003eb.00000000             0       4194304  2
    root@BEAN-2:/var/share/ezfs/shareroot/NAS# 
    root@BEAN-2:/var/share/ezfs/shareroot/NAS# 
    root@BEAN-2:/var/share/ezfs/shareroot/NAS# ceph osd map data 100000003eb.00000000 
    osdmap e49 pool 'data' (2) object '100000003eb.00000000' -> pg 2.afe74fa0 (2.3a0) -> up ([2,0], p2) acting ([2,0], p2)

我们通过dd命令读取该文件：

    root@BEAN-0:/var/share/ezfs/shareroot/NAS# dd if=file_01 of=/dev/null bs=1M 
    4+0 records in
    4+0 records out
    4194304 bytes (4.2 MB) copied, 0.0335059 s, 125 MB/s


很明显osd.2是Primary OSD，因此，读取会从osd.2中读取。osd.2的debug log如下：

    log 是giant版本的ceph

    2016-09-19 18:02:42.338838 7f9aae9fd700 15 osd.2 49 enqueue_op 0x7f9aa796ae00 prio 127 cost 0 latency 0.000105 osd_op(client.19941.1:20 100000003eb.00000000 [read 2097152~2097152 [1@-1]] 2.afe74fa0 read e49) v4
    2016-09-19 18:02:42.338879 7f9ac17f8700 10 osd.2 49 dequeue_op 0x7f9aa796ae00 prio 127 cost 0 latency 0.000146 osd_op(client.19941.1:20 100000003eb.00000000 [read 2097152~2097152 [1@-1]] 2.afe74fa0 read e49) v4 pg pg[2.3a0( v 49'1 (0'0,49'1] local-les=47 n=1 ec=10 les/c 47/47 46/46/42) [2,0] r=0 lpr=46 crt=0'0 lcod 0'0 mlcod 0'0 active+clean]
    2016-09-19 18:02:42.338899 7f9ac17f8700 20 osd.2 pg_epoch: 49 pg[2.3a0( v 49'1 (0'0,49'1] local-les=47 n=1 ec=10 les/c 47/47 46/46/42) [2,0] r=0 lpr=46 crt=0'0 lcod 0'0 mlcod 0'0 active+clean] op_has_sufficient_caps pool=2 (data ) owner=0 need_read_cap=1 need_write_cap=0 need_class_read_cap=0 need_class_write_cap=0 -> yes
    2016-09-19 18:02:42.338912 7f9ac17f8700 10 osd.2 pg_epoch: 49 pg[2.3a0( v 49'1 (0'0,49'1] local-les=47 n=1 ec=10 les/c 47/47 46/46/42) [2,0] r=0 lpr=46 crt=0'0 lcod 0'0 mlcod 0'0 active+clean] handle_message: 0x7f9aa796ae00
    2016-09-19 18:02:42.338920 7f9ac17f8700 10 osd.2 pg_epoch: 49 pg[2.3a0( v 49'1 (0'0,49'1] local-les=47 n=1 ec=10 les/c 47/47 46/46/42) [2,0] r=0 lpr=46 crt=0'0 lcod 0'0 mlcod 0'0 active+clean] do_op osd_op(client.19941.1:20 100000003eb.00000000 [read 2097152~2097152 [1@-1]] 2.afe74fa0 read e49) v4 may_read -> read-ordered flags read
    2016-09-19 18:02:42.338943 7f9ac17f8700 15 filestore(/data/osd.2) getattr 2.3a0_head/afe74fa0/100000003eb.00000000/head//2 '_'
    2016-09-19 18:02:42.338970 7f9ac17f8700 10 filestore(/data/osd.2) getattr 2.3a0_head/afe74fa0/100000003eb.00000000/head//2 '_' = 247
    2016-09-19 18:02:42.338983 7f9ac17f8700 15 filestore(/data/osd.2) getattr 2.3a0_head/afe74fa0/100000003eb.00000000/head//2 'snapset'
    2016-09-19 18:02:42.338991 7f9ac17f8700 10 filestore(/data/osd.2) getattr 2.3a0_head/afe74fa0/100000003eb.00000000/head//2 'snapset' = 31
    2016-09-19 18:02:42.338998 7f9ac17f8700 10 osd.2 pg_epoch: 49 pg[2.3a0( v 49'1 (0'0,49'1] local-les=47 n=1 ec=10 les/c 47/47 46/46/42) [2,0] r=0 lpr=46 crt=0'0 lcod 0'0 mlcod 0'0 active+clean] populate_obc_watchers afe74fa0/100000003eb.00000000/head//2
    2016-09-19 18:02:42.339006 7f9ac17f8700 20 osd.2 pg_epoch: 49 pg[2.3a0( v 49'1 (0'0,49'1] local-les=47 n=1 ec=10 les/c 47/47 46/46/42) [2,0] r=0 lpr=46 crt=0'0 lcod 0'0 mlcod 0'0 active+clean] ReplicatedPG::check_blacklisted_obc_watchers for obc afe74fa0/100000003eb.00000000/head//2
    2016-09-19 18:02:42.339013 7f9ac17f8700 10 osd.2 pg_epoch: 49 pg[2.3a0( v 49'1 (0'0,49'1] local-les=47 n=1 ec=10 les/c 47/47 46/46/42) [2,0] r=0 lpr=46 crt=0'0 lcod 0'0 mlcod 0'0 active+clean] get_object_context: creating obc from disk: 0x7f9ad3533500
    2016-09-19 18:02:42.339021 7f9ac17f8700 10 osd.2 pg_epoch: 49 pg[2.3a0( v 49'1 (0'0,49'1] local-les=47 n=1 ec=10 les/c 47/47 46/46/42) [2,0] r=0 lpr=46 crt=0'0 lcod 0'0 mlcod 0'0 active+clean] get_object_context: 0x7f9ad3533500 afe74fa0/100000003eb.00000000/head//2 rwstate(none n=0 w=0) oi: afe74fa0/100000003eb.00000000/head//2(49'1 client.31094.1:9 wrlock_by=unknown.0.0:0 dirty s 4194304 uv1) ssc: 0x7f9ab23e4e00 snapset: 1=[]:[]+head
    2016-09-19 18:02:42.339033 7f9ac17f8700 10 osd.2 pg_epoch: 49 pg[2.3a0( v 49'1 (0'0,49'1] local-les=47 n=1 ec=10 les/c 47/47 46/46/42) [2,0] r=0 lpr=46 crt=0'0 lcod 0'0 mlcod 0'0 active+clean] find_object_context afe74fa0/100000003eb.00000000/head//2 @head oi=afe74fa0/100000003eb.00000000/head//2(49'1 client.31094.1:9 wrlock_by=unknown.0.0:0 dirty s 4194304 uv1)
    2016-09-19 18:02:42.339052 7f9ac17f8700 10 osd.2 pg_epoch: 49 pg[2.3a0( v 49'1 (0'0,49'1] local-les=47 n=1 ec=10 les/c 47/47 46/46/42) [2,0] r=0 lpr=46 crt=0'0 lcod 0'0 mlcod 0'0 active+clean] execute_ctx 0x7f9ad34cd000
    2016-09-19 18:02:42.339061 7f9ac17f8700 10 osd.2 pg_epoch: 49 pg[2.3a0( v 49'1 (0'0,49'1] local-les=47 n=1 ec=10 les/c 47/47 46/46/42) [2,0] r=0 lpr=46 crt=0'0 lcod 0'0 mlcod 0'0 active+clean] do_op afe74fa0/100000003eb.00000000/head//2 [read 2097152~2097152 [1@-1]] ov 49'1
    2016-09-19 18:02:42.339069 7f9ac17f8700 10 osd.2 pg_epoch: 49 pg[2.3a0( v 49'1 (0'0,49'1] local-les=47 n=1 ec=10 les/c 47/47 46/46/42) [2,0] r=0 lpr=46 crt=0'0 lcod 0'0 mlcod 0'0 active+clean]  taking ondisk_read_lock
    2016-09-19 18:02:42.339077 7f9ac17f8700 10 osd.2 pg_epoch: 49 pg[2.3a0( v 49'1 (0'0,49'1] local-les=47 n=1 ec=10 les/c 47/47 46/46/42) [2,0] r=0 lpr=46 crt=0'0 lcod 0'0 mlcod 0'0 active+clean] do_osd_op afe74fa0/100000003eb.00000000/head//2 [read 2097152~2097152 [1@-1]]
    2016-09-19 18:02:42.339084 7f9ac17f8700 10 osd.2 pg_epoch: 49 pg[2.3a0( v 49'1 (0'0,49'1] local-les=47 n=1 ec=10 les/c 47/47 46/46/42) [2,0] r=0 lpr=46 crt=0'0 lcod 0'0 mlcod 0'0 active+clean] do_osd_op  read 2097152~2097152 [1@-1]
    2016-09-19 18:02:42.339092 7f9ac17f8700 15 filestore(/data/osd.2) read 2.3a0_head/afe74fa0/100000003eb.00000000/head//2 2097152~2097152
    2016-09-19 18:02:42.339590 7f9ac17f8700 10 filestore(/data/osd.2) FileStore::read 2.3a0_head/afe74fa0/100000003eb.00000000/head//2 2097152~2097152/2097152
    2016-09-19 18:02:42.339597 7f9ac17f8700 10 osd.2 pg_epoch: 49 pg[2.3a0( v 49'1 (0'0,49'1] local-les=47 n=1 ec=10 les/c 47/47 46/46/42) [2,0] r=0 lpr=46 crt=0'0 lcod 0'0 mlcod 0'0 active+clean]  read got 2097152 / 2097152 bytes from obj afe74fa0/100000003eb.00000000/head//2
    2016-09-19 18:02:42.339611 7f9ac17f8700 15 osd.2 pg_epoch: 49 pg[2.3a0( v 49'1 (0'0,49'1] local-les=47 n=1 ec=10 les/c 47/47 46/46/42) [2,0] r=0 lpr=46 crt=0'0 lcod 0'0 mlcod 0'0 active+clean] do_osd_op_effects on session 0x7f9aa7811500
    2016-09-19 18:02:42.339619 7f9ac17f8700 10 osd.2 pg_epoch: 49 pg[2.3a0( v 49'1 (0'0,49'1] local-les=47 n=1 ec=10 les/c 47/47 46/46/42) [2,0] r=0 lpr=46 crt=0'0 lcod 0'0 mlcod 0'0 active+clean]  dropping ondisk_read_lock
    2016-09-19 18:02:42.339629 7f9ac17f8700 15 osd.2 pg_epoch: 49 pg[2.3a0( v 49'1 (0'0,49'1] local-les=47 n=1 ec=10 les/c 47/47 46/46/42) [2,0] r=0 lpr=46 crt=0'0 lcod 0'0 mlcod 0'0 active+clean] log_op_stats osd_op(client.19941.1:20 100000003eb.00000000 [read 2097152~2097152] 2.afe74fa0 read e49) v4 inb 0 outb 2097152 rlat 0.000000 lat 0.000895
    2016-09-19 18:02:42.339659 7f9ac17f8700 15 osd.2 pg_epoch: 49 pg[2.3a0( v 49'1 (0'0,49'1] local-les=47 n=1 ec=10 les/c 47/47 46/46/42) [2,0] r=0 lpr=46 crt=0'0 lcod 0'0 mlcod 0'0 active+clean] publish_stats_to_osd 49:35
    2016-09-19 18:02:42.339671 7f9ac17f8700 15 osd.2 pg_epoch: 49 pg[2.3a0( v 49'1 (0'0,49'1] local-les=47 n=1 ec=10 les/c 47/47 46/46/42) [2,0] r=0 lpr=46 crt=0'0 lcod 0'0 mlcod 0'0 active+clean]  requeue_ops
    2016-09-19 18:02:42.339688 7f9ac17f8700 10 osd.2 49 dequeue_op 0x7f9aa796ae00 finish
    
我们可以通过上面的debug打印，跟踪读流程的全部过程。



# 代码分析

## deuque_op fucntion
    /*
     * NOTE: dequeue called in worker thread, with pg lock
     */
    void OSD::dequeue_op(
      PGRef pg, OpRequestRef op,
      ThreadPool::TPHandle &handle)
    {
      utime_t now = ceph_clock_now(cct);
      op->set_dequeued_time(now);
      utime_t latency = now - op->get_req()->get_recv_stamp();
      dout(10) << "dequeue_op " << op << " prio " << op->get_req()->get_priority()
               << " cost " << op->get_req()->get_cost()
               << " latency " << latency
               << " " << *(op->get_req())
               << " pg " << *pg << dendl;
    
      // share our map with sender, if they're old
      if (op->send_map_update) {
        Message *m = op->get_req();
        Session *session = static_cast<Session *>(m->get_connection()->get_priv());
        epoch_t last_sent_epoch;
        if (session) {
          session->sent_epoch_lock.lock();
          last_sent_epoch = session->last_sent_epoch;
          session->sent_epoch_lock.unlock();
        }
        service.share_map(
            m->get_source(),
            m->get_connection().get(),
            op->sent_epoch,
            osdmap,
            session ? &last_sent_epoch : NULL);
        if (session) {
          session->sent_epoch_lock.lock();
          if (session->last_sent_epoch < last_sent_epoch) {
            session->last_sent_epoch = last_sent_epoch;
          }
          session->sent_epoch_lock.unlock();
          session->put();
        }
      }                                                                                                                                                                           
    
      if (pg->deleting)
        return;
    
      op->mark_reached_pg();
    
      pg->do_request(op, handle);
    
      // finish
      dout(10) << "dequeue_op " << op << " finish" << dendl;
    }

其中op->mark_reached_pg 表示，对于该op的处理已经到了reach_pg的阶段。

      void mark_reached_pg() {      
        mark_flag_point(flag_reached_pg, "reached_pg");
      }   

我们dump_ops_in_flight 可以看到当前的OP进行到了哪一步：

    注意例子和read没关系，只是为了展示reched_pg 节点。
    root@Storage2:~# ceph daemon osd.7 dump_ops_in_flight
    { "num_ops": 1,
      "ops": [
            { "description": "osd_op(client.2130451838.0:899198714 rbd_data.2686620486def23.0000000000011595 [sparse-read 4034048~16384] 13.2f7f3fd e34077)",
              "rmw_flags": 2,
              "received_at": "2016-08-03 10:06:13.399398",
              "age": "235.772246",
              "duration": "0.000113",
              "flag_point": "reached pg",
              "client_info": { "client": "client.2130451838",
                  "tid": 899198714},
              "events": [
                    { "time": "2016-08-03 10:06:13.399452",
                      "event": "waiting_for_osdmap"},
                    { "time": "2016-08-03 10:06:13.399511",
                      "event": "reached_pg"}]}]}
                      
## do_request function

    void ReplicatedPG::do_request(
      OpRequestRef& op,
      ThreadPool::TPHandle &handle)
    {
      assert(!op_must_wait_for_map(get_osdmap()->get_epoch(), op));
      if (can_discard_request(op)) {
        return;
      }
      if (flushes_in_progress > 0) {
        dout(20) << flushes_in_progress
                 << " flushes_in_progress pending "
                 << "waiting for active on " << op << dendl;
        waiting_for_peered.push_back(op);
        op->mark_delayed("waiting for peered");
        return;
      }
    
      if (!is_peered()) {
        // Delay unless PGBackend says it's ok
        if (pgbackend->can_handle_while_inactive(op)) {
          bool handled = pgbackend->handle_message(op);
          assert(handled);             
          return;
        } else {
          waiting_for_peered.push_back(op);
          op->mark_delayed("waiting for peered");
          return;
        }
      }
    
      assert(is_peered() && flushes_in_progress == 0);
      if (pgbackend->handle_message(op))
        return;
    
      switch (op->get_req()->get_type()) {
      case CEPH_MSG_OSD_OP:
        if (!is_active()) {
          dout(20) << " peered, not active, waiting for active on " << op << dendl;
          waiting_for_active.push_back(op)
          op->mark_delayed("waiting for active");
          return;
        }
        if (is_replay()) {
          dout(20) << " replay, waiting for active on " << op << dendl;
          waiting_for_active.push_back(op);
          op->mark_delayed("waiting for replay end");
          return;
        }
        // verify client features
        if ((pool.info.has_tiers() || pool.info.is_tier()) &&
            !op->has_feature(CEPH_FEATURE_OSD_CACHEPOOL)) {
          osd->reply_op_error(op, -EOPNOTSUPP);
          return;
        }
        do_op(op); // do it now
        break;
        
        ...
        }
    }

该函数是一个消息处理的总控，根据收到的消息的类型不同，调用不同的函数处理。对于read请求，调用为do_op。

![](/assets/ceph_internals/do_request_switch.png)
                   
                                                                                                                               


